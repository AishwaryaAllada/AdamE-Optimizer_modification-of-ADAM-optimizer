The whole experiment shows the contrast of the decay of gradients of original Adam optimizer to the rate of decay of gradients when finite averaging is performed in the AdamE optimizer. An innovation for faster decay of earlier gradients is implemented following the same principles of Adam algorithm, so the algorithm remains scalable to high-dimensional machine learning problems.
